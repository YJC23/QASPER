{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset qasper (/Users/youngjuchoi/.cache/huggingface/datasets/allenai___qasper/qasper/0.3.0/2bfcd239e581ab83f9ab7b76a82e42c6bcf574a13246ae6cc5a6c357c35f96f9)\n",
      "100%|██████████| 3/3 [00:00<00:00, 253.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"allenai/qasper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.']\n",
      "['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.']\n",
      "{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.', 'It is a ']}\n",
      "{'unanswerable': False, 'extractive_spans': ['seed lexicon consists of positive and negative predicates'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.']}\n",
      "{'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.', 'It is a ']}, {'unanswerable': False, 'extractive_spans': ['seed lexicon consists of positive and negative predicates'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.']}], 'annotation_id': ['31e85022a847f37c15fd0415f3c450c74c8e4755', '95da0a6e1b08db74a405c6a71067c9b272a50ff5'], 'worker_id': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '2cfd959e433f290bb50b55722370f0d22fe090b7']}\n",
      "{'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \\nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.', 'evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. ', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.']}], 'annotation_id': ['1e5e867244ea656c4b7632628086209cf9bae5fa'], 'worker_id': ['2cfd959e433f290bb50b55722370f0d22fe090b7']}\n",
      "What is the seed lexicon?\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"qas\"][\"answers\"][0][\"answer\"][0][\"evidence\"]) # only evidence from answer 1\n",
    "print(dataset[\"train\"][0][\"qas\"][\"answers\"][0][\"answer\"][1][\"evidence\"]) # only evidence from answer 2\n",
    "\n",
    "print(dataset[\"train\"][0][\"qas\"][\"answers\"][0][\"answer\"][0]) # all of answer 0\n",
    "print(dataset[\"train\"][0][\"qas\"][\"answers\"][0][\"answer\"][1]) # all of answer 1\n",
    "\n",
    "\n",
    "print(dataset[\"train\"][0][\"qas\"][\"answers\"][0]) # answers to question 0\n",
    "print(dataset[\"train\"][0][\"qas\"][\"answers\"][1]) # answers to question 1\n",
    "\n",
    "\n",
    "print(dataset[\"train\"][0][\"qas\"][\"question\"][0]) # question 0\n",
    "\n",
    "\n",
    "# print(dataset[\"train\"][0][\"qas\"][\"answers\"].keys())\n",
    "# print(dataset[\"train\"][0][\"qas\"].keys())\n",
    "# print(dataset[\"train\"][0].keys())\n",
    "# print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper Title:  Question Answering based Clinical Text Structuring Using Pre-trained Language Model\n",
      "\n",
      "\n",
      "Paper Abstract:  Clinical text structuring is a critical and fundamental task for clinical research. Traditional methods such as taskspecific end-to-end models and pipeline models usually suffer from the lack of dataset and error propagation. In this paper, we present a question answering based clinical text structuring (QA-CTS) task to unify different specific tasks and make dataset shareable. A novel model that aims to introduce domain-specific features (e.g., clinical named entity information) into pre-trained language model is also proposed for QA-CTS task. Experimental results on Chinese pathology reports collected from Ruijing Hospital demonstrate our presented QA-CTS task is very effective to improve the performance on specific tasks. Our proposed model also competes favorably with strong baseline models in specific tasks.\n",
      "\n",
      "\n",
      "\n",
      "Question:  How is the clinical text structuring task defined?\n",
      "\n",
      "\t Answer:  CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.\n",
      "\t Evidence:\n",
      "\t\t- Quote: Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.\n",
      "\t\t- Quote: However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor cost. To handle it, researchers turn to some rule-based structuring methods which often have lower labor cost.\n",
      "\t\t- Quote: To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# amazing\n",
    "paper_idx = 3\n",
    "question_idx = 2  # within this paper\n",
    "\n",
    "\n",
    "print(\"Paper Title: \", dataset['train'][paper_idx]['title'])\n",
    "print('\\n')\n",
    "print(\"Paper Abstract: \", dataset['train'][paper_idx]['abstract'])\n",
    "print('\\n\\n')\n",
    "\n",
    "print(\"Question: \", dataset['train'][paper_idx]['qas']['question'][question_idx])\n",
    "print('')\n",
    "\n",
    "for answer in dataset['train'][paper_idx]['qas']['answers'][question_idx]['answer']:\n",
    "  if (not answer['unanswerable']) and (answer['free_form_answer'] or answer['yes_no']):\n",
    "    print(\"\\t Answer: \", answer['free_form_answer'] or answer['yes_no'])\n",
    "    print(\"\\t Evidence:\")\n",
    "    for evidence in answer['evidence']:\n",
    "      print('\\t\\t- Quote:', evidence)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
