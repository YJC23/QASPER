{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2c3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'parseJSON.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python3 parseJSON.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49afb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset qasper (/Users/albertomancarella/.cache/huggingface/datasets/allenai___qasper/qasper/0.3.0/2bfcd239e581ab83f9ab7b76a82e42c6bcf574a13246ae6cc5a6c357c35f96f9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154d975b8f3c4051aa84e95d395bcf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "DATASET = load_dataset(\"allenai/qasper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c6f4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Open the JSON file\n",
    "with open('parseOutput.json') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84031836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readQandA(i, response):\n",
    "    print(f\"Article {i}\")\n",
    "    \n",
    "    curArticle = data[i]\n",
    "    print(curArticle.get(\"title\",\"\"))\n",
    "    print(\"\")\n",
    "    for eachQuestion in curArticle[\"qas\"]:\n",
    "        print(eachQuestion[\"question\"])\n",
    "        evidence = eachQuestion[\"evidence\"]\n",
    "        if response == True:\n",
    "            for eachEvidence in evidence:\n",
    "                \n",
    "                print(f\"({(eachEvidence)}) {curArticle['passages'][eachEvidence]}\")\n",
    "        if response:\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0635cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(i):\n",
    "    with open(f\"article{i}.txt\", \"w\") as file:\n",
    "        for i, element in enumerate(data[i][\"passages\"]):\n",
    "            # Add indentation before each element\n",
    "            output_string = f\"({i}) \" + str(element) + \"\\n\"\n",
    "            file.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a2b96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listQuestions(n):\n",
    "    for each in range(n):\n",
    "        readQandA(each, False)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2471cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAbstract(i):\n",
    "    curArticle = DATASET[\"train\"][i]\n",
    "    abstract = curArticle.get(\"abstract\",\"\")\n",
    "    return abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82c11967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQuery(i, questionNum):\n",
    "    print(\"Query:\")\n",
    "    print(\"\")\n",
    "    curArticleData = DATASET[\"train\"][i]\n",
    "    title = curArticleData.get(\"title\",\"\")\n",
    "    abstract = curArticleData.get(\"abstract\",\"\")\n",
    "    print(f\"Title of article: {title}\")\n",
    "    print(\"\")\n",
    "    print(f\"Abstract: {abstract}\")\n",
    "    print(\"\")\n",
    "\n",
    "    curArticle = data[i]\n",
    "\n",
    "    curQuestion = curArticle[\"qas\"][questionNum]\n",
    "    questionText = curQuestion[\"question\"]\n",
    "    evidence = curQuestion[\"evidence\"]\n",
    "    \n",
    "    evidenceOutputArray = []\n",
    "\n",
    "    print(\"Evidence:\")\n",
    "    for each in evidence:\n",
    "        curEvidence = curArticle['passages'][each]\n",
    "        print(curEvidence)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Question:\")\n",
    "    print(questionText)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Answer:\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Write a question self-contained so that a student who is only given the question needs to find the paper and then answer the question. Try not to use the title of the paper itself\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f99285fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5693eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95499306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0\n",
      "Minimally Supervised Learning of Affective Events Using Discourse Relations\n",
      "\n",
      "What is the seed lexicon?\n",
      "What are the results?\n",
      "How are relations used to propagate polarity?\n",
      "How big is the Japanese data?\n",
      "What are labels available in dataset for supervision?\n",
      "How does their model learn using mostly raw data?\n",
      "How big is seed lexicon used for training?\n",
      "How large is raw corpus used for training?\n",
      "\n",
      "Article 1\n",
      "PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry\n",
      "\n",
      "Does the paper report macro F1?\n",
      "How is the annotation experiment evaluated?\n",
      "What are the aesthetic emotions formalized?\n",
      "\n",
      "Article 2\n",
      "Community Identity and User Engagement in a Multi-Community Landscape\n",
      "\n",
      "Do they report results only on English data?\n",
      "How do the various social phenomena examined manifest in different types of communities?\n",
      "What patterns do they observe about how user engagement varies with the characteristics of a community?\n",
      "How did the select the 300 Reddit communities for comparison?\n",
      "How do the authors measure how temporally dynamic a community is?\n",
      "How do the authors measure how distinctive a community is?\n",
      "\n",
      "Article 3\n",
      "Question Answering based Clinical Text Structuring Using Pre-trained Language Model\n",
      "\n",
      "What data is the language model pretrained on?\n",
      "What baselines is the proposed model compared against?\n",
      "How is the clinical text structuring task defined?\n",
      "What are the specific tasks being unified?\n",
      "Is all text in this dataset a question, or are there unrelated sentences in between questions?\n",
      "How many questions are in the dataset?\n",
      "How they introduce domain-specific features into pre-trained language model?\n",
      "How big is QA-CTS task dataset?\n",
      "How big is dataset of pathology reports collected from Ruijing Hospital?\n",
      "What are strong baseline models in specific tasks?\n",
      "\n",
      "Article 4\n",
      "Progress and Tradeoffs in Neural Language Models\n",
      "\n",
      "What aspects have been compared between various language models?\n",
      "what classic language models are mentioned in the paper?\n",
      "What is a commonly used evaluation metric for language models?\n",
      "\n",
      "Article 5\n",
      "Stay On-Topic: Generating Context-specific Fake Restaurant Reviews\n",
      "\n",
      "Which dataset do they use a starting point in generating fake reviews?\n",
      "What kind of model do they use for detection?\n",
      "Does their detection tool work better than human detection?\n",
      "How many reviews in total (both generated and true) do they evaluate on Amazon Mechanical Turk?\n",
      "\n",
      "Article 6\n",
      "Saliency Maps Generation for Automatic Text Summarization\n",
      "\n",
      "Which baselines did they compare?\n",
      "How many attention layers are there in their model?\n",
      "Is the explanation from saliency map correct?\n",
      "\n",
      "Article 7\n",
      "Probabilistic Bias Mitigation in Word Embeddings\n",
      "\n",
      "How is embedding quality assessed?\n",
      "What are the three measures of bias which are reduced in experiments?\n",
      "\n",
      "Article 8\n",
      "Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\\`ub\\'a and Twi\n",
      "\n",
      "What turn out to be more important high volume or high quality data?\n",
      "What two architectures are used?\n",
      "\n",
      "Article 9\n",
      "Is there Gender bias and stereotype in Portuguese Word Embeddings?\n",
      "\n",
      "What were the word embeddings trained on?\n",
      "Which word embeddings are analysed?\n",
      "\n",
      "Article 10\n",
      "Citation Data of Czech Apex Courts\n",
      "\n",
      "Did they experiment on this dataset?\n",
      "How is quality of the citation measured?\n",
      "How big is the dataset?\n",
      "\n",
      "Article 11\n",
      "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment\n",
      "\n",
      "How is the intensity of the PTSD established?\n",
      "How is LIWC incorporated into this system?\n",
      "How many twitter users are surveyed using the clinically validated survey?\n",
      "Which clinically validated survey tools are used?\n",
      "\n",
      "Article 12\n",
      "Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision\n",
      "\n",
      "Did they experiment with the dataset?\n",
      "What is the size of this dataset?\n",
      "\n",
      "Article 13\n",
      "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages\n",
      "\n",
      "what sentiment sources do they compare with?\n",
      "\n",
      "Article 14\n",
      "Word Sense Disambiguation for 158 Languages using Word Embeddings Only\n",
      "\n",
      "Is the method described in this work a clustering-based method?\n",
      "How are the different senses annotated/labeled? \n",
      "Was any extrinsic evaluation carried out?\n",
      "\n",
      "Article 15\n",
      "Spoken Language Identification using ConvNets\n",
      "\n",
      "Is the performance compared against a baseline model?\n",
      "What is the accuracy reported by state-of-the-art methods?\n",
      "\n",
      "Article 16\n",
      "Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data\n",
      "\n",
      "Which vision-based approaches does this approach outperform?\n",
      "What baseline is used for the experimental setup?\n",
      "Which languages are used in the multi-lingual caption model?\n",
      "\n",
      "Article 17\n",
      "AraNet: A Deep Learning Toolkit for Arabic Social Media\n",
      "\n",
      "Did they experiment on all the tasks?\n",
      "What models did they compare to?\n",
      "What datasets are used in training?\n",
      "\n",
      "Article 18\n",
      "Generative Adversarial Nets for Multiple Text Corpora\n",
      "\n",
      "Which GAN do they use?\n",
      "Do they evaluate grammaticality of generated text?\n",
      "Which corpora do they use?\n",
      "\n",
      "Article 19\n",
      "Stacked DeBERT: All Attention in Incomplete Data for Text Classification\n",
      "\n",
      "Do they report results only on English datasets?\n",
      "How do the authors define or exemplify 'incorrect words'?\n",
      "Do they test their approach on a dataset without incomplete data?\n",
      "Should their approach be applied only when dealing with incomplete data?\n",
      "By how much do they outperform other models in the sentiment in intent classification tasks?\n",
      "\n",
      "Article 20\n",
      "Gunrock: A Social Bot for Complex and Engaging Long Conversations\n",
      "\n",
      "What is the sample size of people used to measure user satisfaction?\n",
      "What are all the metrics to measure user engagement?\n",
      "What the system designs introduced?\n",
      "Do they specify the model they use for Gunrock?\n",
      "Do they gather explicit user satisfaction data on Gunrock?\n",
      "How do they correlate user backstory queries to user satisfaction?\n",
      "\n",
      "Article 21\n",
      "Towards Detection of Subjective Bias using Contextualized Word Embeddings\n",
      "\n",
      "What is the baseline for the experiments?\n",
      "Which experiments are perfomed?\n",
      "\n",
      "Article 22\n",
      "Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!\n",
      "\n",
      "Is ROUGE their only baseline?\n",
      "what language models do they use?\n",
      "\n",
      "Article 23\n",
      "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation\n",
      "\n",
      "What misbehavior is identified?\n",
      "Which attention mechanisms do they compare?\n",
      "\n",
      "Article 24\n",
      "Unsupervised Machine Commenting with Neural Variational Topic Model\n",
      "\n",
      "Which paired corpora did they use in the other experiment?\n",
      "By how much does their system outperform the lexicon-based models?\n",
      "Which lexicon-based models did they compare with?\n",
      "How many comments were used?\n",
      "How many articles did they have?\n",
      "What news comment dataset was used?\n",
      "\n",
      "Article 25\n",
      "Enriching BERT with Knowledge Graph Embeddings for Document Classification\n",
      "\n",
      "By how much do they outperform standard BERT?\n",
      "What dataset do they use?\n",
      "How do they combine text representations with the knowledge graph embeddings?\n",
      "\n",
      "Article 26\n",
      "Diachronic Topics in New High German Poetry\n",
      "\n",
      "What is the algorithm used for the classification tasks?\n",
      "Is the outcome of the LDA analysis evaluated in any way?\n",
      "What is the corpus used in the study?\n",
      "\n",
      "Article 27\n",
      "Important Attribute Identification in Knowledge Graph\n",
      "\n",
      "What are the traditional methods to identifying important attributes?\n",
      "What do you use to calculate word/sub-word embeddings\n",
      "\n",
      "Article 28\n",
      "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections\n",
      "\n",
      "Did they propose other metrics?\n",
      "Which real-world datasets did they use?\n",
      "\n",
      "Article 29\n",
      "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016\n",
      "\n",
      "What are the country-specific drivers of international development rhetoric?\n",
      "Is the dataset multilingual?\n",
      "How are the main international development topics that states raise identified?\n",
      "\n",
      "Article 30\n",
      "QnAMaker: Data to Bot in 2 Minutes\n",
      "\n",
      "What experiments do the authors present to validate their system?\n",
      "What components is the QnAMaker composed of?\n",
      "\n",
      "Article 31\n",
      "A simple discriminative training method for machine translation with large-scale features\n",
      "\n",
      "How they measure robustness in experiments?\n",
      "What experiments with large-scale features are performed?\n",
      "\n",
      "Article 32\n",
      "Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses\n",
      "\n",
      "Which ASR system(s) is used in this work?\n",
      "What are the series of simple models?\n",
      "Over which datasets/corpora is this work evaluated?\n",
      "\n",
      "Article 33\n",
      "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German\n",
      "\n",
      "Is the semantic hierarchy representation used for any task?\n",
      "What are the corpora used for the task?\n",
      "Is the model evaluated?\n",
      "\n",
      "Article 34\n",
      "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects\n",
      "\n",
      "What new metrics are suggested to track progress?\n",
      "What intrinsic evaluation metrics are used?\n",
      "What experimental results suggest that using less than 50% of the available training examples might result in overfitting?\n",
      "\n",
      "Article 35\n",
      "Procedural Reasoning Networks for Understanding Multimodal Procedures\n",
      "\n",
      "What multimodality is available in the dataset?\n",
      "What are previously reported models?\n",
      "How better is accuracy of new model compared to previously reported models?\n",
      "\n",
      "Article 36\n",
      "Active Learning for Chinese Word Segmentation in Medical Text\n",
      "\n",
      "How does the scoring model work?\n",
      "How does the active learning model work?\n",
      "Which neural network architectures are employed?\n",
      "\n",
      "Article 37\n",
      "InScript: Narrative texts annotated with script information\n",
      "\n",
      "Did the annotators agreed and how much?\n",
      "How many subjects have been used to create the annotations?\n",
      "\n",
      "Article 38\n",
      "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications\n",
      "\n",
      "What datasets are used to evaluate this approach?\n",
      "How is this approach used to detect incorrect facts?\n",
      "Can this adversarial approach be used to directly improve model accuracy?\n",
      "\n",
      "Article 39\n",
      "Learning Supervised Topic Models for Classification and Regression from Crowds\n",
      "\n",
      "what are the advantages of the proposed model?\n",
      "what are the state of the art approaches?\n",
      "what datasets were used?\n",
      "\n",
      "Article 40\n",
      "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset\n",
      "\n",
      "How was the dataset collected?\n",
      "What are the benchmark models?\n",
      "How was the corpus annotated?\n",
      "\n",
      "Article 41\n",
      "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance\n",
      "\n",
      "What models other than standalone BERT is new model compared to?\n",
      "How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\n",
      "What are three downstream task datasets?\n",
      "What is dataset for word probing task?\n",
      "\n",
      "Article 42\n",
      "Joint Entity Linking with Deep Reinforcement Learning\n",
      "\n",
      "What datasets used for evaluation?\n",
      "what are the mentioned cues?\n",
      "\n",
      "Article 43\n",
      "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b\n",
      "\n",
      "What approaches without reinforcement learning have been tried?\n",
      "What classification approaches were experimented for this task?\n",
      "Did classification models perform better than previous regression one?\n",
      "\n",
      "Article 44\n",
      "Marrying Universal Dependencies and Universal Morphology\n",
      "\n",
      "What are the main sources of recall errors in the mapping?\n",
      "Do they look for inconsistencies between different languages' annotations in UniMorph?\n",
      "Do they look for inconsistencies between different UD treebanks?\n",
      "Which languages do they validate on?\n",
      "\n",
      "Article 45\n",
      "Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning\n",
      "\n",
      "How is face and audio data analysis evaluated?\n",
      "What is the baseline method for the task?\n",
      "What are the emotion detection tools used for audio and face input?\n",
      "\n",
      "Article 46\n",
      "Revisiting Low-Resource Neural Machine Translation: A Case Study\n",
      "\n",
      "what amounts of size were used on german-english?\n",
      "what were their experimental results in the low-resource dataset?\n",
      "what are the methods they compare with in the korean-english dataset?\n",
      "what pitfalls are mentioned in the paper?\n",
      "\n",
      "Article 47\n",
      "Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian\n",
      "\n",
      "Does the paper report the results of previous models applied to the same tasks?\n",
      "What are the causal mapping methods employed?\n",
      "\n",
      "Article 48\n",
      "\"Hinglish\"Language -- Modeling a Messy Code-Mixed Language\n",
      "\n",
      "What is the previous work's model?\n",
      "What dataset is used?\n",
      "How big is the dataset?\n",
      "How is the dataset collected?\n",
      "What models do previous work use?\n",
      "Does the dataset contain content from various social media platforms?\n",
      "What dataset is used?\n",
      "\n",
      "Article 49\n",
      "How Language-Neutral is Multilingual BERT?\n",
      "\n",
      "How they demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment?\n",
      "Are language-specific and language-neutral components disjunctive?\n",
      "How they show that mBERT representations can be split into a language-specific component and a language-neutral component?\n",
      "What challenges this work presents that must be solved to build better language-neutral representations?\n",
      "\n",
      "Article 50\n",
      "CAiRE: An End-to-End Empathetic Chatbot\n",
      "\n",
      "What pretrained LM is used?\n",
      "\n",
      "Article 51\n",
      "Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?\n",
      "\n",
      "What approaches they propose?\n",
      "What faithfulness criteria does they propose?\n",
      "Which are three assumptions in current approaches for defining faithfulness?\n",
      "Which are key points in guidelines for faithfulness evaluation?\n",
      "\n",
      "Article 52\n",
      "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference\n",
      "\n",
      "Did they use the state-of-the-art model to analyze the attention?\n",
      "How many layers are there in their model?\n",
      "\n",
      "Article 53\n",
      "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering\n",
      "\n",
      "What MC abbreviate for?\n",
      "how much of improvement the adaptation model can get?\n",
      "what is the architecture of the baseline model?\n",
      "What is the exact performance on SQUAD?\n",
      "\n",
      "Article 54\n",
      "SUM-QE: a BERT-based Summary Quality Estimation Model\n",
      "\n",
      "What dataset do they use?\n",
      "What simpler models do they look at?\n",
      "\n",
      "Article 55\n",
      "Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction\n",
      "\n",
      "What benchmark datasets are used for the link prediction task?\n",
      "What are state-of-the art models for this task?\n",
      "How better does HAKE model peform than state-of-the-art methods?\n",
      "How are entities mapped onto polar coordinate system?\n",
      "\n",
      "Article 56\n",
      "Machine Translation from Natural Language to Code using Long-Short Term Memory\n",
      "\n",
      "What additional techniques are incorporated?\n",
      "What dataset do they use?\n",
      "What is the architecture of the system?\n",
      "What additional techniques could be incorporated to further improve accuracy?\n",
      "What programming language is target language?\n",
      "What dataset is used to measure accuracy?\n",
      "\n",
      "Article 57\n",
      "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis\n",
      "\n",
      "Is text-to-image synthesis trained is suppervized or unsuppervized manner?\n",
      "What challenges remain unresolved?\n",
      "What is the conclusion of comparison of proposed solution?\n",
      "What is typical GAN architecture for each text-to-image synhesis group?\n",
      "\n",
      "Article 58\n",
      "Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study\n",
      "\n",
      "Where do they employ feature-wise sigmoid gating?\n",
      "Which model architecture do they use to obtain representations?\n",
      "Which downstream sentence-level tasks do they evaluate on?\n",
      "Which similarity datasets do they use?\n",
      "\n",
      "Article 59\n",
      "Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction\n",
      "\n",
      "Are there datasets with relation tuples annotated, how big are datasets available?\n",
      "Which one of two proposed approaches performed better in experiments?\n",
      "What is previous work authors reffer to?\n",
      "How higher are F1 scores compared to previous work?\n",
      "\n",
      "Article 60\n",
      "Learning to Rank Scientific Documents from the Crowd\n",
      "\n",
      "what were the baselines?\n",
      "what is the supervised model they developed?\n",
      "what is the size of this built corpus?\n",
      "what crowdsourcing platform is used?\n",
      "\n",
      "Article 61\n",
      "Exploiting Deep Learning for Persian Sentiment Analysis\n",
      "\n",
      "Which deep learning model performed better?\n",
      "\n",
      "Article 62\n",
      "Talk the Walk: Navigating New York City through Grounded Dialogue\n",
      "\n",
      "Did the authors use crowdsourcing platforms?\n",
      "How was the dataset collected?\n",
      "What language do the agents talk in?\n",
      "What evaluation metrics did the authors look at?\n",
      "What data did they use?\n",
      "\n",
      "Article 63\n",
      "Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks\n",
      "\n",
      "How is the accuracy of the system measured?\n",
      "How is an incoming claim used to retrieve similar factchecked claims?\n",
      "What existing corpus is used for comparison in these experiments?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listQuestions(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dda71b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 71\n",
      "Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models\n",
      "\n",
      "What word embeddings were used?\n",
      "(42) We use the word embeddings for Vietnamese that created by Kyubyong Park and Edouard Grave at al:\n",
      "(43) Kyubyong Park: In his project, he uses two methods including fastText and word2vec to generate word embeddings from wikipedia database backup dumps. His word embedding is the vector of 100 dimension and it has about 10k words.\n",
      "(44) Edouard Grave et al BIBREF11: They use fastText tool to generate word embeddings from Wikipedia. The format is the same at Kyubyong's, but their embedding is the vector of 300 dimension, and they have about 200k words\n",
      "\n",
      "What type of errors were produced by the BLSTM-CNN-CRF system?\n",
      "(16) Step 2: Based on the best results (BLSTM-CNN-CRF), error analysis is performed based on five types of errors (No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag), in a way similar to BIBREF10, but we analyze on both gold labels and predicted labels (more detail in figure 1 and 2).\n",
      "\n",
      "How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?\n",
      "(47) Table 2 shows our experiments on two models with and without different pre-trained word embedding – KP means the Kyubyong Park’s pre-trained word embeddings and EG means Edouard Grave’s pre-trained word embeddings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first input represents article index\n",
    "# second input represents if to include response for each question\n",
    "readQandA(71, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3d30211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has been shown that word embeddings derived from large corpora tend to incorporate biases present in their training data. Various methods for mitigating these biases have been proposed, but recent work has demonstrated that these methods hide but fail to truly remove the biases, which can still be observed in word nearest-neighbor statistics. In this work we propose a probabilistic view of word embedding bias. We leverage this framework to present a novel method for mitigating bias which relies on probabilistic observations to yield a more robust bias mitigation algorithm. We demonstrate that this method effectively reduces bias according to three separate measures of bias while maintaining embedding quality across various popular benchmark semantic tasks\n"
     ]
    }
   ],
   "source": [
    "print(getAbstract(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfb26057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "\n",
      "Title of article: Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models\n",
      "\n",
      "Abstract: In recent years, Vietnamese Named Entity Recognition (NER) systems have had a great breakthrough when using Deep Neural Network methods. This paper describes the primary errors of the state-of-the-art NER systems on Vietnamese language. After conducting experiments on BLSTM-CNN-CRF and BLSTM-CRF models with different word embeddings on the Vietnamese NER dataset. This dataset is provided by VLSP in 2016 and used to evaluate most of the current Vietnamese NER systems. We noticed that BLSTM-CNN-CRF gives better results, therefore, we analyze the errors on this model in detail. Our error-analysis results provide us thorough insights in order to increase the performance of NER for the Vietnamese language and improve the quality of the corpus in the future works.\n",
      "\n",
      "Evidence:\n",
      "Step 2: Based on the best results (BLSTM-CNN-CRF), error analysis is performed based on five types of errors (No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag), in a way similar to BIBREF10, but we analyze on both gold labels and predicted labels (more detail in figure 1 and 2).\n",
      "\n",
      "Question:\n",
      "What type of errors were produced by the BLSTM-CNN-CRF system?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Write a question self-contained so that a student who is only given the question needs to find the paper and then answer the question. Try not to use the title of the paper itself\n"
     ]
    }
   ],
   "source": [
    "generateQuery(71,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
